test the transformer model we found on huggingface to see if it can work
if not, run through the google colab
instead of letting it generate stuff try to steal images from bluesky?

https://huggingface.co/google/vit-base-patch16-224
google vision transformer come back and refresh on this once in a while so you can talk about it


maybe i should fine tune the ai model myself
no, I don't want to fine tune something myself I want to take advantage of some existing libraries


use firesky.tv hose of data to look for posts with [images]

need to get set of followed accounts
need to get database set of checked accounts


?? maybe ?? keep a set of accounts that have been screened - store these in a table that gets booted up every time 
maybe like a three strike policy? what's the odds that I check the same account more than once? guess it depends on how long it's been running

check if the account that posted the image is already being followed

now that we know it's a valid new account to check
run the image through an ai computer vision image classifier - determine whether or not the image is a cat.

?? maybe ?? if it is a cat check if there's text in the image


if the newly posted image is a cat then let's just assume they're a cat poster, like the image, follow the account, put in the database that we added this account for this post id and tested positive for cat = true


maybe we could have an account status monitoring - 
change in followers and follow count since last time ran,
check and see if any of the new followers were followed by the auto process so we can count gains from the bot



also need to hook it up to my google photos bucket of ricky pics and post a new one every day
what should the caption be?
:3, double heart, 


https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/feed/cats
cats feed to use

at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.generator/whats-hot
this can replace the did -> cats part but you have to replace the app.feed.generator to just feed to use it in a url to see the page.

so if we use 
https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/app.bsky.feed.generator/cats
we should have a feed generator for our cats to be sourced from.

other cat feeds
caturday - https://bsky.app/profile/numb.comfortab.ly/feed/aaad4sb7tyvjw
handle - @numb.comfortab.ly

Siamese cats - https://bsky.app/profile/mrfenman.bsky.social/feed/aaac6wmikqyhq
handle - @mrfenman.bsky.social

cat pics - https://bsky.app/profile/jaz.bsky.social/feed/cv:cat
handle - @jaz.bsky.social 


db schema
table - followship
account | follow date | follow post | followed back | removed from friends
account id should be a primary key and indexed as a hash key 
| user_id | handle | follow_uri | follow_date | follows_you

max handle length is 300 chars so we're going to use that as max length for our varchars

table - cat checks
account | post # checked | was cat | was not text | followed?
account id should be a primary key and indexed as a hash key 
#nvm all that
| post id | seen_date | 

table - friend count and date
date (pk) | followers | following | engagement count on most recent 5 posts

because as of postgres 8.something it finally is WAL approved and works better than b-tree for its intended use case. When to consider using a hash index:
Primary key index: If you need extremely fast lookups on a unique primary key column. 
High cardinality, unique data: When most values in a column are distinct and you primarily perform equality comparisons. 
this is an exact description of what we need it for lol
however, we're going to be using Pandas to parse our data not the database itself, so we don't need all of that. 

Workflow 0 - initialize followership db:
for everyone in the friends list, add username, today's date, no post, and mark whether they are also following you, 
add all followed users to both tables

Workflow 1 - follow new catposters:
1. pull current posts from feed
2. check for posts with images
for each post {
3. see if the poster is already being followed/already was in db, if yes bail on this poster
4. run image through cat checker ai, if no bail on this poster
5. run image through text checker ai, if yes text bail on this poster
6. hooray! this is a real cat pic and not just a meme someone posted or something (hopefully). Follow the poster. Add username to the database of people, the date added, and the post, mark as false for followed by.
}

Workflow 2 - reducing follower/followed by ratio:
while followed by ratio is less than 35% (followed by / followed by + followers) {
1. scan through friends list, update relationships between followed by so that we don't delete anyone that has followed us back.
2. query the database for the last 10 people, are not following, and have not been removed from friends
3. mark them as removed from friends and unfollow them
}

Workflow 3 - posting:
# bluesky actually doesn't have native post scheduling which is very annoying because how am I supposed to post in bulk with a program you only run once without just clicking to run it every time you want to?
anyway...
# also need to figure out how to connect to the shared ricky album with the google photos api maybe? 
https://developers.google.com/photos/picker/guides/get-started-picker
otherwise we don't have any content to automatically post and this is a bust.
0. link ricky pics google drive folder
1. create set of pics and used pic ids
2. randomly select a pic, select one that hasn't been used already
3. post it.
4. mark as used
5. set up cron job for next time sometime random between 12 and 24 hours


step 5 - https://github.com/Zeeshanahmad4/Bluesky-Post-Bot 
details how to set up a cron schedule for GitHub posting. would need to make it private so people don't have access to my bluesky and google photos account, and it's kind of a security risk but whatever.


because of the google photos api complication that I don't want to deal with yet, let's get an mvp off the ground that only focuses on growth through followership, and I'll do the posting myself.
then once that's firmly working we can look into the other part. that'll give more time to think of hashtags and 


prioritized backlog - 

phase 1 - set up main ai photo recognition base and test bluesky functionality
1. set up colab
2. set up the bluesky code to get into the #list
2. figure out how to run that ai model
3. run both ai models on a sample of like 100 pics to get started
 - - have it pull 100 pics from today's view of the #list
4. see how the reverse image search works
5. set up liking pictures and following accounts on bluesky account
6. give it a spin

phase 2 - set up sql database
1. set up tables
2. initialize bsky friend analysis
3. add database updating to base bluesky functionality from phase 1
4. test 

phase 3 - friend pruning
1. create panda - sql queries to get updates to list - does it need to do this every time? oh yeah it's gotta check for follow backs.
-- with hash index lookup this is an o1 operation, and even if we have oN where n is our friend list size, going through even a billion friends wouldn't be that bad I think. So we can just do this every time.
2. calculate acceptable followership condition by given variable of x where is the % of followers to followers+following
3. [for if unacceptable ratio] write sql queries to get most recent x friends where x is the difference that will get us back up to a [acceptable]% follower ratio
4. use that query of people necessary to delete to get to correct ratio, and delete all the people from bluesky as necessary, when successful update table with new status, return all rows updated
5. test

phase 4 - logging and testing framework
1. write function to check current stats followers and following, need to check the most recent 5 posts and count up engagement. count a like as one engagement point and a repost as 50 engagement points.
2. update the other functions to run this stats updating in the relevant sql table when we run all the code
3. set up a bit of unit testing for each phase to show that it's all working correctly.

phase 5 - cron job automation
https://github.com/Zeeshanahmad4/Bluesky-Post-Bot
1. time to figure out how to get this process to automate so you can just have it run every night or something.
2. need to set up secret keys in GitHub and run it and see if everything is still working as intended.
3. get it to do everything fast one day to see it all in action, then set it to run.

phase 6 - dockerization???

phase 7 - LOGGING - 
1. automatic loggers that get posted to gethub project every time something gets run
1b. needs to show friends before action, friends after action, follows added/deleted

need to come up with way to go to second page - limit is 100 on feed requests.


post id, date checked
need to begin initialization with deletion of any posts older than three days old so we don't hold that



i'm trying hard to think about this workflow again
0. the database exists
update friends db?
1. pandas pulls our two tables of posts and ids, creating a set of s


I think we only care about our follows, not our followers - I don't really care how many bots have followed us in the previous week. 

follower ratio = len(client.get_follows('me').follows) / len(client.get_followers('me').followers)
following from get follows = client.get_follows('me').follows[x].following
see if they followback = client.get_follows('me').follows[x].followed_by


Pull up pandas, create map of user : follows_back
use client.get_follows('me') to pull up all users I am following
for each user in the get_follows results
	see if they are in the pandas set
	if they are not, add them to an add list with the information you need for the table
	if they are make sure that they still have the same follows_back status
		if they have changed add them to an update query list
after running through each user in the set 
run update query
run add query
run update query - any friends added that don't have an add date should be set to today


ok i can't figure out how to get the pyscopg2 connected because i can't figure out how to get the database set up to accept anything
eventually I'll have to ssh in
need to watch video on ssh even though you used to do it all the time you just did it for work without understanding a lot.
 

now that sql queries are getting generated successfully
need to {
  0. write queries to update friends list
  1. do deletion code
  2. do post database, and pull it into memory
  3. figure out what's causing the two-posts-same-did issue, maybe it takes a second for follower to go through?
	-- create a session set to check for
}

WE'VE OFFICIALLY DOUBLED!! - 2:21 pm 01/20/2025
jan 17 - jan 20 72 hrs
9am to 5:01pm 8 hrs

61 days = 1464 hours

487/1464 = 
0.3326502732240437 fol/hr

487/80 = 
6.0875

6.0875 - 0.3326502732240437
/ 0.3326502732240437
 = increased rate 17.3 * 100 = 1730%

really need to do figure out duplicate protection - here are some things I'm noticing. It's always just duplicates from the same query, it's never failing because the id already exists in the db, but because there are 2 duplicate ids in the query. So it simply must be because we can follow twice before the api 'realizes' we've already followed that user. Just need a temporary store of users for one query at a time, the issue is never that there is one user from a query we just did. although that would probably be better implementation just in case. How would we purge the set? well maybe just timeout is enough because we don't mind having that set open for infinity, we want all of our followslist in a set anyway.
OH I GET IT. the reason was because you make the query of all these feed posts at once, and while that is made it says that all of the users that aren't following you aren't following you. So when you follow one of the users, the feed pages that you've already loaded aren't updated. So the set fix is perfect.

the CATPICS feed is really not pulling it's weight lol could investigate some new feeds. Siamese is doing much better than I thought it would

deletion code without being able to pass the sql directly into python and parse with pandas is proving to be a tiny bit more of a chore than I'd hoped.
Luckily pruning doesn't occur very frequently, but for this immediate manual implementation I basically have to just have the query that gets all the follow IDs I want to remove, ctrl+H (replace) the at:// with , at://, and turn each line into an array to iterate through. 
Then you have to run another query to do the deletions.

Still need to write some code that will add users in that I have been adding through regular app usage. Similar to the initialization of the db code, but instead I need to write some to compare the db to the current friends list and make updates.
I think that part can wait until direct db connectivity is established to the code. Otherwise there's going to be a LOT of querying.

So for the deletion step
1. get list of friends in db with current friend status
2. search through follows and see if they follow you to update status
3. output update query with all users that need to be updated.

DELETE FROM "bsky"."follows"
WHERE user_handle = 'e1341.bsky.social'
RETURNING *;

DELETE FROM follows
WHERE follow_id IN (
    SELECT follow_id
    FROM follows
    WHERE follows_you = FALSE
    ORDER BY date_added ASC
    LIMIT 100
);


1/23/2025 - encountered my first error. Looks like someone deleted their picture or their whole post after I had generated the feed, and so when I tried to use the link to the image the source wasn't found and it caused a bad html request. So I added some very basic error handling.
But now that means I have ~50 new friends that aren't registered in the database.
It's time to create a function to update the friends list and see which followers have been missed.
This will probably be an update to the update friends function but definitely a little different. 
Need to pull the database somehow and register it as a... map? Maybe I need to actually use some pandas but building a dataframe out of text input seems like overkill.


need to rewrite the delete query to auto-parse the follow links instead of using notepad++ to add quotes.

part 4 needs to be implementing the post-cacheing feature, there's a lot of duplicate work being done checking posts from 1. earlier in the day and 2. posts that make it into multiple feeds.
The most important question is how big should our cache be? I guess I don't mind it growing to be about 5000, it would be the same as copying and pasting the friends list. Maybe before updating the cache you delete all but the last 3 days of posts? maybe even only 2 days?
for logging we could even set it up so that we input the post cid and the date we checked it, so when we log that we've already checked that post we can say if it was one that we saw this batch, or saw on x date. Normally that would be kind of overkill but here I think it's important to let us know how far back we need to save posts. Ultimately we might not even need to save more than like the past 12 hours. But without the timelogging there's no way to be sure.
That should be another statistic that we keep track of in the logging.

In fact, I need to start thinking about what kind of logging we want because that's the next thing to work on.
Stats we should log - Date of run, follow+follower count before. which feeds were checked, how many posts were checked. # posts with vids, # posts without pics, # errors, # pics processed, # cat pics observed, # people followed, # mutual posts seen, # already followed seen
total from all feeds run
For logging if the file gets above a certain size how do we initialize a new file?

then follow+followers after, ratio, and if we're still in acceptable territory
then if we're not it'll go through the deletion routine when we're fully automated, and it'll log that. 

should I actually log all of the handles and uris added? is that too much? or do we just want stats?

just had a weird error where I pulled up my own picture and tried to follow myself maybe? that's weird. Need to add some handling for that lol. To clarify it didn't error the program, but caused a sql error because I'm already in the database. But we're not pulling the db until friend pruning, only checking the follow and followback, so I guess we need a check to make sure that the user is not you in case you 
post a pic, and then start following people

great! logging works on the main add-followers part of the routine. now need to do logging for the updating followers and the deleting followers routines. 

could use a little effort in refactoring too - duplicated constants (datetime just got cloned)
constants moved up to the top?
once we get closer to full automation we'll need to do the full refactor, like making the queries prettier. so worry about that later, but get that datetime copy out

1/30/25 0100
the logging works great, and the caching works great! The performance improvement is incredibly obvious, being that it reduces the time from n*m to oN time, we no longer repeat reprocessing posts and that's like 9/10 of the time is processing the image.

The gains of followers obviously feels like it's slowing down. I think I'm still getting like 70 a day but it feels like a crawl. 
I have two ideas for how to improve this:
1. we create our own feed, and expand search terms. instead of just looking for cat, cats, Siamese, we look for all breeds, we look for words like kitty, kitten, and just trust our ai to weed out the nonsense.
2. more frequent but shallow friend searches. Instead of trying to do this twice a day at 2 and 8, maybe we need to do like 4 times a day at 08, 1300, 1700, 2100. We can just do a couple hundred but it'll be more likely to grab people that are actually on. 
The problem with this style is if we move forward with google cloud scheduling, they charge 10 cents per run, and this would be like 120 runs per month instead of 60. but maybe that's just a pay to play live with it kind of deal. or maybe we just set up the cron job on the laptop. Maybe I can rig it to an older laptop that just hangs out open all the time. 

now that logging and caching are complete though, we're getting to the point where all we have left to do is automate. So I think I need to start investigating connecting to the sql database from the internet now. Progress might be slowed, but at least we have the logging feature to feel good about and watch some progress for now, I wouldn't even mind taking a break to focus on some other work again for a week. 

Also side note on just the feelings of this project, it feels weird like I'm stressing about 70 followers gained in a day like it's not good enough. that's pretty insane actually.
I was worried like my picture today not reaching 100 likes meant I wasn't doing something right. But it has 90 likes that's awesome too.
I even made an unrelated mtg post just now and it already has 10 likes, better than almost any twitter post I ever made before this project.
The human condition normalizes the dopamine you get every step of the way and asks you to reach for more. But try to keep some perspective here. Everything is sweet.


The next direction I want to go is to explore replies to posts. Replies with cat pics are gold because that's the kind of user you want to be friends with. High engagement. 


2025-01-31 03:30 -
I realize now that if I'm not getting to the prefiously cached posts, then maybe I'm not reaching out to enough people. There must be a lot in the gaps that I'm not checking. And now it doesn't cost to put like 1000 posts for each feed, because any duplicates won't get checked.
I still think that 4 per day should be the way to go, but I should see how many it takes to get to ones that I've seen already
also should change logging to note diff between cached and seen posts.
2025-01-31 15:20 -
Went 1000 posts back this run, and it did not hit any cached posts from the run 12 hours ago.. So that means we can push back further. My goal for tomorrow (caturday) will be to check every post lol.
The one great part about the caching is if you change your mind and want to go back further than you just ran you won't hit those that you just did. Otherwise it seems very difficult lol.

Really need to implement reply reading next, like I was talking about before. that's gonna be huge.

2025-01-31 1630
I was scrolling through my Siamese cat feed when I saw a picture that perturbed me - a Siamese cat like ricky, no alt text, with 514 likes from yesterday. The user has less than half my followers and follows fewer people than that. 
But they do have 2400 posts, where I only have a few less than 200. They must be doing a LOT of replying.
My first instinct was to just go through and follow everyone who liked that cat post.
Maybe it's a mistake to try to follow Content creators. I want to get the people that ENJOY the kind of content I'm putting out.

What I need to do is find a way to do a graph traversal, whenever one of these cat pictures I encounter is popular, we have to go through all of the people liking those posts and start following THEM.
drum up more engagement on our own posts.

Also I want to create a function for the logging so that I can create a customized field where I can say like "we're updating our friends list today because I liked 500 users on my app instead of through rickybot"
but it has to be like... not permanent I don't want to accidentally set all of the logging functions to that so I won't do it for now.

But new goals
1. like likers
2. like replies

we still check if the post is in the cache
but before we skip it due to it being a mutual follower post or someone we already follow post check the number of likes that the post has.
	if it is over say 10? 25? then we check if it's a cat post regardless of if we follow that person or not and do like-based-following.
		but obviously if it's not a cat post then we skip it

This is a fundamental change in strategy, and though it will ruin the "timeline" because I won't have a perfectly cultured feed of cat posts forever, I will improve engagement.
Should I go to the cat-liker and like one of their posts before following? probably not. who knows what they're posting, after all.
but i guess if they show up in the cat feed then they'll get some mutual likes.

It's also time to discard other feeds. We only need Caturday + 1 cat type feed. Siamese Cats will be our primary.

Should write a query that groups entries in the follows table by the date_added and follows_you cols, so we can count the number of people from every batch that have and haven't followed
count follows you, count doesn't follow you, and % of followbacks from this batch

2025-02-01 01:01
I realize now that liking all likers of a post can lead to "friend bombs." 
I'm trying to keep a reasonable number of follows from posts and wasn't planning on capping follows from likes, but now i'm seeing posts with like THOUSANDS. I know I want more follows but how many more? hm.

2025-02-01 02:51

2500 Deletions got back to weds
need way to keep track of the db stats


2025-02-01 0423
ok maybe i pushed a little too far
got rate limited
so that's not good
but more importantly it totally borked the whole notebook and messed up my logging. Ran an update-db check to get the new users in. I was excited that my logging was working and I was watching the progress seeing all the ? emojis pop up. Very good visual warning lol. In the except clause I need to add a little more handling to bail out early if we go over like 10

"Under this system, an account may create at most 1,666 records per hour and 11,666 records per day. That means an account can like up to 1,666 records in one hour with no problem. We took the most active human users on the network into account when we set this threshold (you surpassed our expectations!)."
https://docs.bsky.app/docs/advanced-guides/rate-limits
Action Type	Value
CREATE	3 points
UPDATE	2 points
DELETE	1 point


2025-02-01
stop liking myself in the post likingfollows
since we have to make queries to get replies THAT is a reason to cap it to only popular posts, probably only worth it with like 25+ likers

2025-02-01 1220
690 worked no problem.
if you have 11666 and 1666 per hour that is 7 runs. if you have 7 runs in a day that is every 3.5 hours
so theoretically an optimal workload for the day would be <max amount of likes in 1666 points of CRUD> * 7 [at 0100, 0900, 1200, 1500, 1800, 2100, 2400]
now we just need to push to see what the max requests you can do in an hour really is.
1000 broke but I think that's because it was our second batch in an hour because I was testing code.
690 worked good this morning.
Next time we'll push to 1000 to see
I'm thinking that even if the liking posts drops down the rate limiting from like 1666 follows to only 1000 or 1500, that's still probably more than we can get from how many actual catposters we can follow in 3 hours, which would probably be like 500 maybe? But actually the reads are not create, update, or deletes so they shouldn't score any more points, it should still be 1666
hopefully the like queries are taking less than I thought and we can get pretty close to 1500 follows per batch.
I suppose instead of testing with 1000 next I should test 1500 next to make sure the rate limit is being handled ok now that I have that error handling break.
Let's stress test and do a full 1669 to make sure we hit the breaking point

Once we're habitually running maxxed out posts the cache will actually be useful, too! making sure we don't repeat work for real. I stopped pasting the queries in but I'll try it on this run that has only 2 hours of gap to see if it's useful in best case scenarios

looks like even if a post has 227 likes I'm only seeing the first 50. must be another page system, gotta fix that handling
tally up likers who were already followed/following and log that at the end of that function

ok wow the db cache actually worked a little on that stress test run

but we got 1201 new follows in our stress test run. not sure how many that was from liking posts and mutuals posts. 
need a successful run on the logs I accidentally forgot to put the error array in my join and blew it. next time though.
you really don't want to hit rate limit though, because then you can't like and respond to users on your phone. bad for engagement.
second stress test got 1140 new follows, definitely liked a lot more mutual posts because I was going through a lot of the ones I error-parsed last time.
The error handling is finally fixed and will successfully return out of the function and log now even if bad things happen.

when doing runs hour-to-hour the db cache was actually clutch. Extremely clutch. stopped processing like 6 pages of entries once we got towards the end.
which made our stress test not actually be so stressful, but that's a good thing. don't want to get rate limited for no reason lol.


ALWAYS NEED TO set up deletion day on Friday. Saturday should be adds only. We want to max growth on Saturday 100%, so that means no spending any of our CRUD point allowance on deletions.
1000 points on two thousand deletions was 333 follows we couldn't get to. but that's fine 3888 - 333 brings it down to 3555 for the day

finally got a successful stress test off!
error handling worked perfectly
1322 new users was max, with 32 mutual likes and 317 likes on the cat pics themselves
= 1671 actions. Pretty damn close to the the 1666 estimated.


automation - need to find some way to make sure we always have enough API power.
if we get a rate limit schedule update for an hour after, delete an hour after that. Or something, just gotta prevent jobs from running within an hour of each other.


how can I automate the statistics table? 
I want to pull this query to keep recording the percent of obtained followers at different times.

Time for a big combination of both documents.
I want to create a "settings" bar at the top 
how much I want to run the program, which feeds to run, whether or not to update, and whether or not to delete
The only thing is I really need to get database connectivity sorted out or I still have to do the copypasting for the cached posts and for the deletions. So annoying. 
If I could get the database connectivity I wouldn't even need to do copypasting at all. hm. Maybe I should reprioritize that.
Still, I think we almost always want to update our database shortly after running these queries so I'll at least bring that in and set some variables at the top in place of the parameters I keep changing.

2025-02-02 1607
when i was on a walk i realized that I do want to make the stats table. 
It should have date, # added total, # followbacks current, and followback% at sub2h, sub6h, sub12h, sub24h, sub48h, sub72h, sub1week, sub1month, and ending% After a week any non-followers will probably be purged, with our new higher rate I don't think people will last longer than a few days, really. when a date is purged then we get the final ending% value of followbacks. 


2025-02-03 0939
I keep thinking about this problem of automation, and I was wondering of if I should even have any self-hosted parts at all. Perhaps it would be better for my development as a developer to put the aws practices I'm learning into effect instead of the sql ones.
I was looking at this service that lets you automate a jupyter notebook, but I keep trying to do the math on the runtime 

Maybe I need to use the timer magic on one page of 100 posts in a feed. run that as many times as possible and see how much runtime it takes. 
Or maybe my logging I can add another timer to the logging before it posts so I can see how much cpu time it will take.

Regardless I'm thinking that maybe if I containerize the function to preserve the bsky and ml imports, we can turn this script into a lambda function on aws so that it's easy to automate.
Then if I used dynamodb instead of aws rds I could be entirely serverless and that would be pretty cool.
The only thing is it would discard the database components as they are in favor of a NoSQL version.
And that would require a bit of refactoring, because I've learned that dynamodb runs on apis to delete key-value pairs, and that the max you can do in a batch is 25, so it isn't really prepared to process huge loads of multiple thousands.

But here's how I'm thinking we refactor.
Instead of storing the entire database of friends in dynamodb, we store RUNS in dynamodb. we have a key which is the date, and the value is an array of dids that we added during that run. A set amount of days after that run completes, we pull that entry's array, check every follower to see if they're following now. and if they're not, delete them. Pruning based on batch date rather than on a query to get the oldest users from the db.
The only things that we would have to account for better is 1. getting human-added users into a batch somehow, and 2. picking up on the random one-offs where they followed during that first week but then later unfollowed.
Maybe like once a week we have a full overhaul of the friends list and see how everything is doing? A super-batch of sorts. Every week we create a superbatch of every follow we have, and then the following week we see how that went and create a new superbatch of what's left next. 
If we're doing that we almost don't need to worry about the other days as much.
Maybe one deletion update can happen every night to cover the follows from exactly 7 days prior.

Migrating to AWS will definitely solve the issue of our self-hosted db communication issue. 
But to implement it first we need to figure out if this containerized deployment of the routine to a lambda function will work at all. So I'll try to get what we've got put into a lambda function for now. Maybe the lambda function can email me the outputs with amazon sns so I can paste the queries in. 
We'll focus on automating there, and then once we know that we CAN do it that way, I'll incorporate dynamodb and we'll redo the lambda once that works.

i think that this kind of solution should scale better. We know our max scaling is capped by the bluesky api rate limiting now. 
Caturday is a power day, and the other days can take it easy. That way we can average out our usage over the course of the month (aws bills by month).

this would also solve the issue of rate limits being encountered by deletions too. It takes much longer, looks like about 3930 instead of 1700 (not sure how the rate limiting math works there, should still be 2 points instead of 3 but i guess without the post liking that adds up a lot) but I managed to accidentally stress test without thinking on 4500.
Curious what happens when we run the same batch of deletions in an hour when our rate limit unexpires but we've already half-processed them.
Suffice to say we should only delete 3500 users in a batch max in the future from now on. Added error handling to the deletion part too though so that's good.

Well I think the program didn't actually fail due to rate limiting, there was actually a None value in the list that caused an error at 3930, exactly where our program stopped. I left the None value in this time and it appropriately caught the error. 
I do need to fix the logging to somehow see how many people we actually deleted though.

And also need to add a second logging time to the end to see the end times for everything! Gotta start measuring time.

So to solve the dynamodb problem for the other table, posts, the posts cache.
what we'll store is just a single row, which is the posts we encountered in our last post. That way the size doesn't grow exponentially, it only is one read and one write every time, and we store it pre-parsed into an array already. Ezpz


this run of 403 posts 691 follows took 7:00.2087, which is 420 seconds.

aws gives 3.2 million seconds of compute time per month free
gb of compute time is measured by how much memory you're using too, so I need to start tracking the memory of my program so I can get an impression of how it's growing per follow+posts

2025-02-03 2209
OHH! I just had a brilliant idea of how to handle some of the stragglers. If someone unfollows we shouldn't keep them in the pool and reintroduce them to a new batch, instead we should add those people to the delete query and just unfollow them ourselves because there's no way they're coming back. That means all we need to account for is the people we're adding manually.

maybe it's backwards to containerize and then set it up to work with dynamodb and then recontainerize.
dynamodb should be an easy hookup now that we have things planned out.

2025-02-07 0149
been taking a break from updating the program so that I can focus on learning AWS concepts, primarily lambdas and how to dockerize this project so we can automate that way.
Will need to create some memory benchmarks so that we can get an initial impression of how much memory our program needs. We might be able to do it through playing around though, Mostly assuming that we'll be comparing 1gb vs 2gb. Not really worried about the memory limits itself, but rather that the cpu allocated to us will be better with higher mem and that might help run the computer vis.

I'm also thinking about the math of how we should be running it.
Since we don't really need to limit the cron jobs anymore, we could do as much as 1 every half hour if we really wanted to. I'm not a fan of that idea because I think it will cause too much overlap.
I think 1 per hour on caturdays, and one every 2 hours on not-caturdays.
Will need to look how EventBridge does scheduled asynchronous invocations for the lambda function, and see what kind of info I can pass in there. Perhaps it will be able to say what feed we're using, but might need to just handle that in code the way I'm doing it now. 

so let's think about maxes
11666 per day, 1666 per hour. 1322 new users was max, with 32 mutual likes and 317 likes on the cat pics themselves
= 1671 actions
11666/1666 = 7.002400 MAXED OUT runs per day. So if we want to do 4 runs per day we could simply divide the max out by 4, which should give us enough of a buffer since 24/7 is ~ 3.5
instead of looking for 1322 follows, we'll look for 300, which will also give us plenty of buffer for likes.
So every hour on caturday we do 300 follows, which will gain us 7200 follows.
every day except Saturday we'll have a deletion time that gets rid of everything from older than one week prior.
non caturdays we should do maybe like 250 every 2 hours? that's 3000 per day and should give us plenty of room to do our deletions. Friday will be sketchier, but we just need to schedule the deletions on an off-hour

2025-02-07 1215
going to try to stress test deletions again. trying a batch of 5000 to see how it goes
my logging on the deletion function really needs to be improved. 
I did hit the rate limit, but for some reason it doesn't register deletions like that.
went from 12934 to 7953 and it said successfully deleted 5000, so I never hit 5 rate limit errors, but when I tried to do some follows afterward to see how much ratelimit space I had left, it was already ratelimited.
the max deletions you can do in an hour is 4981, it seems.
I tried to add some printing to show what I'm deleting and now i'm getting rate limit errors on the deletions and it's picking up, but idk why it didn't during the run. weird.

if we don't manage to get through all the users in a deletion queue we should save an array of them to be deleted at the next possible time.


1. need to refactor for this deprecation warning about 
FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.  warnings.warn(
2. should I change timing to see how long the setup takes to get an impression of how long the lambda will actually be charging?
-- nah this will either work or not lol
3. still thinking about how many runs to do on caturday
1322 * 7 = 9250 per day /48 = 192.8
I think instead of doing 300 every hour maybe we do 175 every half hour. the closer we get to when people are posting the better. 
smaller batches will be better in case we encounter errors as well, and will decrease the size of the previously seen post CIDs being processed.
4. FIGURE OUT HOW TO DIFFERENTIATE SUCCESS AND FAILURE WHEN REMOVING FRIENDS


I think maybe I need a lambda function that will collapse all the follows obtained from each run in a day
and join them together?
or maybe you can just append all the new values to a db entry
then when deletes happen it will just take the biggest chunk it can out of that array
but deletes will be more like updates, because we check the status of everyone we followed

so there will be two stores:
weekly update store of all friends - this allows us to check for sneaky people who stop following us once a week
daily update store - all follows added from runs on x date, then deletions will be processed 7 days later
except for caturday runs which will be processed 10 days later instead. 
still trying to think of an elegant way to do this. 
we could have 14 buckets and check the corresponding bucket, at the end of every day you turn the current day bucket into the previous week's bucket that could be something
the caturday handling is still annoying though, but we really HAVE to maximize our gains on this day.
so Monday is deletion day, Monday will check Saturday, and at the end of every Monday new Saturday turns into old Saturday and new Monday turns into old Monday. we'll just have to add extra handling to include Saturday in the Monday handling.

14 things in dynamodb seems very manageable. and instead of loading and appending every time, we can add a timestamp for the run as a key inside the new_day key, then an array of all the people followed.
At the end of the day when we're converting new day into old day, THAT's when we'll combine all of the new runs into a single array of follows to be stored in the old_day key directly
so
old_monday : [followers, added, the, previous, monday]
new_monday : 2025-02-10-0123 : [followers, added, this, run]
		2025-02-10-0532 : [followers, added, this, run]

I also like this because rather than matching some non-specific timestamp date everything will be easier to reference as a key-value pair. Provided that there's a way to iterate through all keys of the dynamodb which obviously there should be.
Also instead of having delete day be Monday, I think we can combine Friday-Saturday into the delete day.
We'll just make sure that the update combines everything end of Saturday instead of end of Friday. 

the previously seen cached posts are so useless lol i almost want to scrap this feature.


There is a new error that's been popping up today, I suppose it was the same one that broke my function once and inspired me to wrap the getfeed in a try catch, but what's interesting is there's no error message to it. Stackoverflow told me to use repr(e) to get the error type. So at least now I can see that it's an InvokeTimeoutError. But not sure what to do about that. Why would it time out after going through 11 pages? idk but my code is handling it and i'm not totally upset by the early finish. Especially when we convert to shorter runs it will be ok.
just good that we're logging it appropriately in case it becomes a bigger deal in the future.

to stop storage from cropping up with stats I want to make another Logging file just for stats on GitHub that we'll push to every time that we finalize the weekly stats of a batch. like after 7 days we see how many followbacks we got, logging the percentage and then allowing that data to be deleted, getting it out of the dynamodb capacity.
Should I use auto-expiration to save on write costs? probably doesn't matter with our limited level of throughput, but we Could that's good to know. Since we have the write capacity unutilized though, I think we should wait to delete until we have for sure processed our data.

Ok I'm watching this course on dynamodb and they're really emphasizing not to get really big attributes in there.
I think dynamodb will still be perfect for the set of post ids from the previous run. Should never be bigger than like 500 and we really need speed to make that efficient.
I think DynamoDB might be a good place to store the resulting follows as well.
But when we combine the resulting follows together and hold them until the following week we should probably store that in S3 instead. Especially since friday+sat will get up to like 13000 follows maybe. Split Friday and sat into two different dynamodb keys and just combine them together into s3
So we'll have an even better microservices architecture -
1. main function adds follows, reads and writes last seen set of posts to dynamodb, writes new follows to dynamodb
2. weekly checkup writes current follows to s3, compares last week's friends from s3 to current follows+followers
3. daily combiner takes daily run data (follows added) from DynamoDB and combines it into a larger set of daily data to hold for one week
4. weekly stats processing and deletions takes s3 data of follows from 7 days ago, checks how many follows followbacked, deletes non followers from bluesky, deletes data from s3 and puts output to GitHub as logging.


need to write an alert in the logging if ratio goes too high 
and write a logger line for what the percent is at the final cur followers:now following statement.

the part 12345 system I've got going in GitHub needs to change, need to do a final upload of this part (3 & 5), then part 6 will be where I break the files up into their microservices - 
the add followers is good where I can run all the steps and it's a self contained program, that will be easy to convert to a python program and lambda-fy it.
So I need to do that with the Update service, as well as the Delete service (which will include an update service at the beginning.

things to look up how to do 
1. plug in aws services into the functions in all of the spots where we're inputting or outputting our PostgreSQL queries/results.
2. dockerizing for lambda, how to upload
3. getting the secrets in and out of secretkeeper or whatever that one's called
4. disconnecting runtime when finished

timing time it takes to update might take too long at like 1000000 follows if we only have 15 mins

need to add logging stats how many followers we had at the time of the end of the week.
like when we delete Mondays we say how many followers we had before and after so we can record it on the % capture followback to compare to

2025-02-11 0414
I've had what I would call decision paralysis in deciding what my priority for this migration to AWS services should be. I keep getting hung up on the process of lambdaizing and eventbridge and docker, when I really need to just keep getting things done.
I was reflecting on it once again just now, and had a burst of insight on how to proceed.
Rather than trying to do the complicated parts first and then filling in the easy parts, I need to just get everything I CAN do done. 
What that means for me is the next phase of this project is microservice-ization and incorporating dynamodb and s3 where I want to.
I can use the api services that connect you into those storages and incorporate them into my current workflow while still running my program manually - I was kind of inspired by the AWS recommendations on migrating to dynamodb by transitioning your database by connecting new writes to dynamodb and then once the service is live moving the data over.
The hangup with dockerization is that pytorch is big and we need to do a containerized lambda for the photos. But pytorch and our ViT are ONLY used in the photo identification code. We don't need to worry about containerized lambdas for our microservices to 1. check follows list from last week and make sure nobody unfollowed us 2. delete users purge 3. combine the daily runs into s3 storage to purge
So what I want to do next is create a version of this that starts to do all of the aws stuff. Have the runs export to dynamodb. write the microservice to convert dynamodb to s3. get our delete queries from s3 and delete them daily. Those don't need to be containerized and should be easier to set up in lambda. Which means that I can then learn how that normal process works, get some experience there, and set up those functionalities with eventbridge to practice using that.
Then the final piece of the puzzle will be the final conversion of dockerizing the main gain-follows routine, and using what I already know to set up eventbridge to get it to start going by itself.
this is doable. I can do this. Clear path and priorities.

First I need to get rid of this none mismatch issue in the update code it's messing up my logs. how do I keep getting myself in this damn database

40000 lines of text is getting very unwieldy to paste into colab, too. it's time to move on from this system of updates for sure lol
add timestamping to delete service
check notes for other to do items

Side note I'm very happy to be decoupling the deletion process from the update process - having to recheck your full friends list 3 times a day for these days where i'm purging 10500 people is really obnoxious.
Also i'm glad that we're going to be deleting users in batches, because when we partially delete a batch it makes the stats look a little funky on the bottom row. not a big deal there though.

2025-02-12 0222
looks like getting rid of the deprecation error was ezpz. luckily i had the old cat testing colab intact so I could just plug in the new package and it worked exactly like it did before, 100% same results.
The last big fix I need to do is get rid of the authentication warning from huggingface. supposedly logging in gets you a higher rate limit and might have better download speed, so the last thing I want to fix before microservicing is getting that set up correctly.


2025-02-12
NEED TRY CATCHES AROUND FOLLOWING+FOLLOWERS
literally anything that has an api call. 
Had a bad gateway error just now 

2025-02-12
wow fixing the huggingface and vit warnings were literally the easiest things to do ever lol. shouts out to deepseek and chatgpt.
Moving on to the unofficial phase 6 today. - microservices.

https://chatgpt.com/canvas/shared/67ada1671f608191b452635f1df5c034
tried to see how close chatgpt could get me to a step by step we'll see how correct it is if I just follow it.

note to self - when you refactor to only have one block of seen posts, make sure you keep the cids of any posts that are seen again in the next run so they're not forgotten. This will enable 30min runs if desired

2025-02-15 2359
today I painstakingly load tested the Saturday runs. Did a run almost every hour, sometimes half hours or 45 mins to try to make sure we didn't overdo anything. In all I followed just over 8000 people, and gained ~1000 new followers. 
I think the insights I've gained are 1. I never want to do this many runs manually again. 2. following back closer to the time that people post isn't necessarily going to improve the followback rate dramatically. 3. the numbers I previously stated should be good without overdoing anything. I think it'll be hard to hit the cap every week actually. 4. half hour and 45 mins are unnecessary, doing a run every hour will be plenty sufficient. In fact I'm considering going way down on the main days, maybe like once every 3 or 4 hours instead. There just aren't enough posts to merit running the program that long. 5. I think my strategy of keeping just the most recent seen posts will be sufficient, provided that I make sure that I code it to keep any cids that are seen in the current run (even if they were previously seen). 6. once you have 45000 lines of text in a cell it starts to take a little bit for your ram to handle it in browser lol. I'm really excited to automate this process as well. Definitely going to try to take a break from updates and deletes until we try to start automating.
1 in 8 day of is a pretty good rate, especially since many of the follows were in the recent 4 hours. All in all I'm pretty satisfied.
Up to 9200 followers, almost ready to crack that mythical 10000 that chatgpt said would make me a low level influencer. That's exciting.

Started setting up the aws stuff, I really need to get moving on it. Realized that it would be easiest to transition by using an AWS User not a role for the transition code, and then the role will be for when I convert it into a lambda function. Hopefully will make some good progress on that tomorrow.

I also want to make a few logging files - one for regular logging, one for a general follow update to show how many people we're adding and gaining per day, and one for deletions that will show the final stats of followbacks when we remove that day of batches. 

bugfixes still need to fix - make sure you don't follow yourself from likes, and make sure every single api call to amazon and bluesky and GitHub are all wrapped in try catches. 

man, llms are soo good at pulling up the simple code I just don't know like how to use AWS apis and convert python code to json and back. What a lifesaver I feel like this is speeding me up so much. It's almost too easy lmao

getting a weird api response error from the .get_feed() function. There's some weird response. ChatGPT says it seems like a change in the bluesky api since it's not anything in my code, which makes sense. It's just weird that it's happening all of a sudden. And it always happens on the same page - it happened when I went to the second page, now that I've waited 30 mins it's letting me get through 2 pages and erroring on the 3rd. so strange.
I'll leave it for now and focus on the other things I'm working on, but might need to play around with something if this error keeps popping up:
ModelError("1 validation error for Response\nfeed.15.post.embed.`app.bsky.embed.images#view`.images.0.aspectRatio.$type\n  Input should be 'app.bsky.embed.defs#aspectRatio' 
but since it's atproto chatgpt says it should be fixed soon lol. 
2025-02-17 - yeah looks like it's not happening anymore. That's good. Even if this issue pops up again it shouldn't make a huge impact on our performance since we're going to be doing tinier runs in the automation.


so creating the aggregator to convert the dynamodb entries into s3 was pretty easy. But of course I couldn't really test it too easily because there's no data for it to process. So now I just need to do the other parts now that I know for sure that I can get the aws stuff plugged in.

2025-02-17 
Getting ready to write my status update code, I realized there's a small hangup.
so the point of this one is to get a status check on our current follows and followers
I know that I want to delete any followers that unfollowed, because they won't be in any buckets to be checked in the future and the likelihood of them re-following after unfollowing is probably incredibly small.
But what about manually added follows? Do I want to add those to the prune list? Should I add them to their own bucket to be checked? If they're just cat accounts I don't want them to sit there forever. And I don't want the code to delete people I actually want to follow. 
I guess I just don't want to get rid of manual follows, and I'll have to just stop following people manually. I've noticed the followback rates on my manual follows aren't really any better anyway.
so our status checker will just be a weekly check to make sure nobody has stopped following us

2025-02-18 0528
Looking very goooood. Just did an overhaul to the Add Follows code. It's not tested, but it looks sound. I think I've really done a good job of making sure all our api calls are wrapped in try-catch blocks. And i'm not sure where following ourselves was coming from, but I've discarded that from the finalized set so even though we may not have fixed that bug it won't impact us anymore.
The last piece to build is the deletion code! Should be able to get that done today, and then I can look into lambdaizing the code. Although that might need to wait until next week -- not only do I want to give this a few runs to make sure it isn't buggy before we totally automate it, I also think that it will be easier once I set up my own laptop, which is coming in next week. So I'll try to set up my dev environment and then figure that out.
I'm very excited now. I guess I always have been lol.

Current follow stats, last update was 9806, and that was our 31-day monthiversary. We took two months to reach 487 on Jan 17, so in one month (Feb 17) we got 9319 followers.
So we can multiply 9319 * 2 and divide by 487 to get our increase in rate
9319/1mo vs 487/2mo = 243.5/1mo
(9319 - 243.5) / 243.5 * 100 = 3727.10% rate increase

2025-02-19 0128
theorycrafting how many times we have to run our delete function every day so we don't waste a lot of lambda runs. in theory it shouldn't ding us too much but we don't need to run 12 deletion runs a day if we're going to use 6 tops
if our max number of followers added is (9319 * 2) 18638, and we can delete 3500 per session, we need at most 5.5 sessions per day if we're MAXING two days. which we're not. there's one day where we'll get max (sat) and one day where we will not (Friday)
but we're actually going to get 8400 followers on Saturdays at 350 per hour
and we're going to get 4800 on Fridays at 400 per 2 hours
if our max follow count is on the fri+sat days and we get 13200 on those, with 3500 deletions that only takes 3.77 runs to get through them all, giving us the flexibility of going up to 14000 follows, which means we could go up to 375 per hour on Saturdays (9000) and 416 on weekdays (4992) OR 
350 on Saturdays still (8400) and 466 on weekdays (5592)

plus I really doubt that we'll ever be hitting perfect follow counts, and I've already set up the aggregate function to keep any leftovers in the s3 bucket if there are any, so nothing to worry about there. 

The one downside of doing things this way is that our stats are VERY muddled. we don't get to see how we're performing per-run at all, and theoretically with holdovers we might even really see them per day.
Is there a way we could fix that? 
Our singular deletion runs will basically be logging just X/3500. 
I suppose every day we could use the dynamodb to keep track of the running totals: we have attributes for followbacks, deleted accounts, total followed
and then when we finish processing the S3 for the day we can do an extra logging function that summates the stats for the day. I like that.
I was kind of hoping to keep this function without accessing dynamodb so it would be lighter, but it really doesn't matter because the only reason we were splitting off to s3 was for the weight of having 30000 strings in a set in dynamodb at once. 
Ok so we'll make a del_stats key similar to how we have the cache key, and we'll keep track of relevant stats - and this del_stats key will be its own day of the week so that it doesn't get accidentally deleted by the aggregator doing it's thing
ALSO I just realized our projected deletion count will be WAY enough to cover everything with 4 runs, because we're processing ~14000 FOLLOWS, and not all of those follows need to be deleted.

2025-02-19 0708
it appears that we made the right decision implementing s3. I ran the aggregator and had a successful first run, which is great. 
Went to the AWS portal and checked my s3 bucket and saw that it was successfully converted, which is awesome. While I was there I saw the file sizes - the 4 runs that gained 2151 followers yesterday takes 75.5 kb. However, the map of our followers takes a whopping 898kb, which makes sense as it's something like 24000 * 2? 
400kb is the max you can keep in dynamodb, which definitely wouldn't have worked for us. It also explains why our chrome was starting to crash out, trying to copy and paste like a megabyte of memory lol
However, I'm also a bit concerned about our max run size. 
If we get to 14000 I think we'll be at about 500kb. DynamoDB file size is measured by Item not by Attribute, so even though we've split up our runs I think we'll be out of luck by the end of Friday+Saturday. 
I'm trying to think of a better solution but we already almost have one implemented - just in case of leftovers in the s3 bucket I did write the code so that we could insert on top of it.
So theoretically I could just empty out Friday's follows into the fri+sat bucket, and free up the rest of our dynamodb memory for Saturday.
If we set up the cron job for 12:30 and make sure Saturday's runs don't start until 1am, then that means that we'll also get a perfect little snapshot of how well Friday performed (not in conversion rate, but simply in follow acquisition).
So the more I think about it the better it sounds. We just need to run the aggregator one more time to pre-test before fri+sat, if we run the code now it should see the stuff already in s3, add nothing to it, and readd it. and yes! we have 0 runs but a total of 2151 follows. Sweet.
So i'll just get rid of the check to not run on Saturday, and turn off the warning that the s3 should be empty on Saturday
theoretically we could have just created another item for Saturday, and then had the aggregator take care of both later though. hm.

2025-02-20 1208
so far runs are going very smoothly for the 
-- i don't remember what I was going to say there.
Thursdays run was 2800 follows and is 101kb of data in our s3 storage. if that is a constant, then we reasonably have about 10000 follows that we can put into dynamodb without any risk of it crashing i'm feeling comfortable about our new setup being able to handle the 8400 from Saturday
I really want to start lambdaing. Even without deletes fully tested and without adding being figured out with containerization, we should be able to lambda the aggregate and status update functions to run on their routines.

2025-02-22 0137
CRUCIAL SETTING - I've written all my code in colab, and I was planning on copying it into vscode or something to edit it, but realized there Had to be a download as py file in colab. Which there is.
But pasting my code from colab to aws has everything garbled because aws expects 4 whitespace spaces per indent and colab by default only uses 2.
so before you download make sure you change that setting.
OH MY GOODNESS I NEEDED TO SET THAT BEFORE i STARTED WORKING NOW EVERYTHING IS STUCK I'M SO WRECKED
lol nvm i just actually need to install vs code i'll be fine

2025-02-22 0459
HOLY SHIT YEAH IT WORKED LET'S GO!! 
FIRST LAMBDA-IFICATION IS COMPLETE
had to fix timeout issue and memory allocation because I forgot the settings start at minimum. but the second run worked! first was 0420, retries at 0423 and 0427 because they timed out at 3m, but the second run at 0431 was a success! Confirmed with the GitHub logging and the table.scan() of what our dynamodb looks like rn.

I need to write a reflection about how crazy it develops your brain thinking about the real specs of the programs you're working on. I never really think about what size my code is but even this small-ish file is 1mb. Really curious about what the add followers part looks like.

after a year's worth of commits I should to apply to tesla lmao. I'm really excited once the other microservices go up the commit history will really light up to near max.
maybe I should do a repurposed one where I do the same thing but with twitter.
then we Really start looking successful. multiple platforms? potential of getting paid? and if you can fully automate both then hell yeah. 
it's kind of funny that now bsky is free and x  is paid, so it's really an effective strategy to hone everything in on bluesky and then try to merge the code over to .
their pricing model - 
Free Tier: Very limited, only allows posting a few tweets and reading 1,500 tweets per month.
Basic Tier ($100/month): Allows 50,000 tweets read and 3,000 posted per month.
Enterprise Tier (Custom Pricing, Very Expensive): Needed for large-scale applications like analytics, large data pulls, or managing multiple accounts. OK i've saved those stats in another file, but the strategy would have to dramatically shift - only following 400 accts per day even if you spend $175 per month it's a total scam. They go by read count, $200 per month to get 3000 reads it's crazy.

The reason I needed to write right now is because it feels SO good to have been worrying about this process for WEEKS. studying what's going on with all these aws services. And now I do have at least one whole microserve fully automated where I won't have to think about it ever again unless it fails and I have to manually run one.

I'm trying to look up how many posts we view per day so that I can adjust to twitter and it is a little annoying hand querying.
wish I could code it into an excel sheet tool.

you can't scan ALT TEXT on X. That's the most tragic thing that really makes it the better experience as a developer. also finding posts through feeds to avoid reading posts directly like you can on bluesky because it seems like the best way you'd be able to do it is ration a certain amount of posts per day read, 15000 reads per month is 500 reads per day. 
right now we're doing like 3 batches of 1000 and getting 1000 follows per run, and we're usually seeing 600 posts and getting 1000 follows
so we could do like 500 per day for one run and try to get our 400 followers, and spend $200 a month to do that lmao.
maybe it's best not to use chatgpt to summarize documentation lmao
is 100 per month for 10000 reads any better? jeezy peezy. anyway that'll be something to worry about later if you're interested. Hate to see such heavy restrictions 

2025-02-22 1106
had our first mystery error Idk what happened but it looks like we did a whole run and just didn't add them. So that's a little weird. It was our 4th run of the day not sure what happened. I woke up like an hour later and couldnt salvage it so I guess we'll have to adjust the update status code to take care of it
so strange though, there wasn't any kind of error at the end of the add-users cell that was running it just stopped at 84 for some reason. Then the other cells didn't run. Pretty weird maybe it was just a colab issue.

how could we make the status update code a little more efficient so we're not checking the profile of like 30000 people. I guess I'm just a little worried about overlapping with the delete code?

oh hey looks like having all my colabs up at once finally bit me in the butt - I ran the status update code, making 91 successful deletions and 55 failures. So probably just a timeout issue. Also why does my update code not have an early termination set up that's no good.

Good thing we ran into this before lambdaing it. Definitely need to rework just a bit.
if we hit errors we should add them back into the old followers so we can see to delete them later? or will follows just handle that itself.

so we need a full list of follows to see what we have every week. then we need to look back to last week's list and see if that follow existed last week. If so it's a full 7 days old and is worth checking. We'll already have our current followers up in a set I believe, so then we can cross reference to see if they're a follower without making more bluesky api calls other than the delete. That's pretty sick.

looks like 9300 follows are 323.8kb on range so we should be good there confirmed.

whelp. I'm feeling a lot les confident about this second one. Who knows what's going to happen. The code is largely untested because I haven't had several weeks to test it from week to week. But I'm pretty confident that I adapted the update code well. 
-- encountered an error with the first deployment, turns out my package is busted. Possibly because I tried to install atproto into the same folder as yesterday and one of the installed folders had an overlap that got messed up.
So don't try to reuse layer folders lmao no building off them it takes like 5 seconds anyway.

The filename _pydantic_core.cp313-win_amd64.pyd breaks down as follows:
cp313  Built for Python 3.13.
win_amd64  Built for Windows x86-64 (AMD64), NOT ARM64.
So, this is not an ARM64-compatible build. If you're running Python on Windows ARM64, you need to reinstall it correctly.

keep hitting this pydantic core error turns
0 17 is cron for noon

2025-02-24 0937
finally figured out what was going wrong with the docker build image. someone on reddit posted a similar issue when building with windows and the comments said to use the providence flag
docker build --provenance=false --platform linux/arm64 -t docker-image-name .
this command worked for me and we have our first successful docker image!
there is one thing that I'm a little concerned about - in my aggregate unction I see a logging warning that there were objects in the s3 bucket, and I added an if statement so it wouldn't create that warning if yesterday was Friday. But it should have been? So something's wrong there. OH. it's because yesterday will only have stuff in it already if yesterday was SATURDAY.

deletion cron job
cron(0 0-10/2 * * 0-5)
nvm needed to reformat for aws because they do chars for days of the week instead of nums
Also it turns out they can't do ranges like that, you just specify hours
cron(0 5,7,9,11,13,15 ? * SUN-FRI *)
(this is midnight, 2, 4, 6, 8, and 10 translated from est to utc)

cron(0 4-23 ? * SAT *)  # 05:00-23:00 UTC on Saturday (also does 11pm friday to account for time zone shifts
cron(0 0-4 ? * SUN *)   # 00:00-04:00 UTC on Sunday

man everything is so wonky because of the utc issue. 
made a nice little graph of how I'm going to schedule all the jobs and space them out to hopefully never hit the action caps.
after all the planning here are our cron jobs for adds:
cron(0 0,2,4-23 ? * SAT *)
cron(0 0-4,6,8,10,12,14,16,18,20,22 ? * SUN *)
cron(0 0,2,4,6,8,10,12,14,16,18,20,22 ? * MON-FRI *)

2025-02-24 0800
alright! moment of truth!
In one hour we'll have our first scheduled cron job add follows run. I'm so excited holy cow. I might never have to manually run these follow runs again 
whoops forgot to set the run settings oops. 11pm
Now this is a heavier cpu function, so I'm going to allocate 2gigs to hopefully get a good enough cpu that it runs faster. I'd be open to running a little more, but only if it takes like 10 mins or doesn't boot up fast enough.

ok so I messed up with the aws variables, forgot to change them to not use colab. So fixed that. but was also getting some timeout issues, so I'm just gonna crank up the memory to max to see if we can get a successful run off first, then downshift as necessary (it says you can pick a number up to 10240 but erorrs if you go over 3008. So trying with 3 gigs.)

all set up with a new image and more memory gonna force a test real quick

2025-02-26 0216
oh man it took SO much work trying to get the Add Follows function to work. Claude gave me some better direction in what I needed to do, but what's crazy is youtube still the goat for random helpful info. Someone showed me how to local download the ViT and incorporate it into a docker image, so I used Claude to do that with mine (since mine was a different kind of model than the vid). LLM's do AWESOME work with using logs to spit out what the issue is with a program. lmao sometimes I get a little text blind or I'm not sure which part is the important part looking at a wall of logging and they just see it instantly.
I guess I need to practice that.
But in any case I've got the add follows lambda set up and it's on a cron job now! never going to need to add follows againnnnnnnnn.

Now it's just the deletes to do! I could do some testing rn but I almost want to just launch into croning it. Seems a little risky but what's the worst that could happen?

2025-02-26 0509
let's think about some math.
I'm using 3gigs for my add follows routine. so that means instead of the 400,000 free gbs we only have 133333s. 
our runs are looking like 4 mins + 1 min of startup time = 60*5=300s
133,333 / 300 = 444.44 / 31 = 14.33 per day that we can do and still end up in free tier.
So this is outside our budget, we'll end up spending money. Let's see if we can go down to 2 gigabytes.
If we can go to 2gb and still end up at ~6 mins we'll be better off for it
billed duration is only 3 mins and 4 mins so that's good!
our memory used in mb is 1300 and that's with a max 1000 seen posts. so def dropping down to 2048 from 3008 mb of memory allocated to see how performance goes next time.
555 / 31 = 17.921, so with the 13 per day fom deletions and adds, we get 4*6 to go towards the caturday runs, 24, and then we have 17 left over for the update and a few deletions and the aggregation runs. I think that might be a sweet spot of barely not spending money. But that depends on how long these runs take. Might need to cut down on the run count and increase the length of the runs?

luckily after the free tier the next charge is First 6 Billion GB-seconds / month	$0.0000166667 for every GB-second.
which means the second amount of 400,000 GbS is only $7. That's not so bad if we go over.

2025-02-26 0619
Alright, the delete code is dockerized and lambdafied. Feeling a bit nervous about this one because it's harder to test and we won't know how it handles fri+sat until we get there, but any big fuckups will get covered by the status update service so nbd.
Fingers crossed! Today is our first official day of full automation now. That's crazy. It's been a long ride lmao. I'll be updating this with notes about the runs as I observe them for the rest of the week at least.
Just in case you ever have to update a docker here're the commands lol 
# log in to aws
aws ecr get-login-password --region <REGION> | docker login --username AWS --password-stdin <ACCOUNTNUM>.dkr.ecr.<REGION>.amazonaws.com
# create aws ecr repo if necessary
 aws ecr create-repository --repository-name <AWS REPO>
# build docker image in linux compatible format for arm64 chips
docker build --provenance=false --platform linux/arm64 -t <DOCKER IMAGE NAME> .
# tag the docker image to the aws repo
 docker tag <DOCKER IMAGE NAME>:latest <ACCOUNTNUM>.dkr.ecr.<REGION>.amazonaws.com/<AWS REPO>:latest
# and push updates to the aws repo
docker push <ACCOUNTNUM>.dkr.ecr.<REGION>.amazonaws.com/<AWS REPO>:latest

Great work, btw. Proud of you :*

run 1 at 5am was 3.5m with 3gigs, 250 posts checked
run 2 at 7am was 5.5m with 2gigs, 300 posts checked.
so yeah, we just need more memory. Can't be helped.
so we're gonna be looking at that 133,333 number, might end up paying a monthly sub to aws like they wanted. but it'll be worth.

first automated deletion run was a success, running 3.5 mins. Just need to observe that we bail out of the 3 subsequent 'empty' runs and i'll be satisfied that the weekly runs are good to go.
However I'm starting to think that for the add follows the best thing would be to just cut the runs in half for not-saturday. Reducing to 6 runs might save 2 mins per run of startup time, which would be like 6*27. We'll see, I want to at least observe how it runs like this for about a week unless our usage chart is like insane

run 3 at 9am was 3.30m with 3gigs, 238 posts checked

for the empty delete runs we're looking at 5.25 seconds. that's a quarter of a minute totally no big deal I like that.
Alright!

as for the other runs, the longest wa 5.5m still, there was another of those, the average is 5mins, and the shortest is still 3.2 mins. 
I'm making MASSIVE GAINS today though. 
started today at 13288 followers, and now we're at 13572. A 500 follower jump is practically unheard of. that's like half of caturday numbers!!
Looks like following people shortly after they post might really be making a difference.

FOR STATISTICS SAKE 
FULL LAMBDA AUTOMATION BEGAN 2025-02-26 01:43:54.33
prior followers: 13288 | previously following: 33284
So we can measure growth at like 1 week, two weeks, 1 month from there.

next time you need to upload an update I realize it would have been way better to name the logging files with no number, and and there could be a fifth microservice that migrates that over to a back, or just manually copy it over to a new file and delete everything from the old one leaving it with the same name to start filling up again.

followers at 9pm -  followers: 13732
followers at 11pm - followers: 13760
followers at 01am - followers: 13801
so in the span of 24 hours gained ~500 followers.

wow that means we could do a time-to-500 like how we compared to the original manual run
61 days * 24 hours vs 24 hours is nuts
6100%? it would simplify down to that right. or 6000% that number feels less cool though actually than 3172 lmfao

I need to create an aws desktop what is the name of those again 
create one with the display of all the services running, recent costs for each, successful runs of each

2025-02-27 1328
runs are going really well.
I posted a picture during "dead hours" - the only runs that haven't been able to hit the full 400 are at 3am, 5am, and 7am. Posted a pic at 3:45 last night and it's only up to 465 likes today. Fascinating that it feels like that is "underperforming" now (another pic posted at 11:30am already has 525 likes). 
But there's been some interesting analysis on the dead hours. Even though I'm only getting 200 new follows during those runs, I'm still getting ~40 follow-backs in between each run. So it may be possible that I'm hitting a higher conversion rate during that period. It almost makes me wish that I had things set up to see the conversion rate per run instead of per day, but I think I have plenty of statistics as it is.

need to clean up the unused ECR registry before 12 months so I'm not getting charged for all the test images

2025-02-28 0827
good news and bad news, the good news is that it appears my code handled rate limiting well. The bad news is that for the first deletion run of friday only 3000 deletions occurred instead of 3500, meaning we got rate limited well before.
For now I've disabled the add followers runs - 9:00 won't occur, deletions will occur at 10:00, we'll get through 3500 deletions of the 6216 remaining users, and it will take the third deletion run at 12 and I can turn on follows again at 1.

THIS is when I need to do my dead zone runs. Instead of having 12 + 4 a day I need to do like 9+3 and 8 + 4. Literally just skip a few and prioritize the deletions having enough space to not get rate limited.
For the normal weekdays actually we could do like 10 + 2 + 2 extra checks
for fri sat should definitely do 8 + 4.
and the deletion runs will occur during 1am 3am 5am 7am est since those are the runs failing to find 400 new follows 1am is the closest with 366

5 and 7 have never managed to get all 400 follows. 3am sometimes, 1am most of the time, 9am all the time

So the checks will be in between we should have enough to not get rate limited there
i.e.
now deletions will be at 3, 4, 5, 6, 7, 8 on fridays
5, 6, 7, 8 on s-th

best case/expected case is that 7 and 8 will be empty runs no matter what, and then we'll have plenty of time to transition back to normal runs so we don't get rate limited. Worst case is we still have people and it carries over. 

To see if this will work I'm going to run another deletion at 9 so that we have a delete every hour for this fri+sat.
Hopefully the only reason we got rate limited is because follows chew into the 5 min range and so we still had part of our max rate per hour gobbled.
Maybe it would be better to give some buffer on fridays of an hour and a half.
0300, 0415, 0530, 0645, 0800, 0900 seems like a good schedule for fridays actually, but that doesnt fit into a cron job very well at all lol you'd need 4 different triggers.
we'll see how this 9am run goes right now, and if we need to space the runs out a little further then so be it, i'll set up whatever amount of triggers I have to.
quick prayge

now that we have a second run at 9am that also maxed at 3005 I think that there's an issue with our delete cap. We're maxing out at 3005 users consistently per hour, so it's probably irrelevant to the add runs beforehand. So the 75 mins won't help. I think I need to adjust the code to stop at 2750 users processed.
Because the crucial thing is that instead of straight up feeding a list of deletions, now we're costing Reads to our bluesky allotment. reads cost 1 point per, which I was disregarding before because they weren't actually interacting with us.

So I have two potential solutions to refactor the delete code - 1. we do a pre-delete status check of all the users in the s3 bucket so we can efficiently delete 3500 deletions every time. 
2. reduce this to 2750 and have 8 runs per day on fridays to be safe. (2750 will cover 14000 in 5.45 runs)
3. do nothing just let it max out every time. - I don't like this because it messes up the logging.

I think we can do 2750 and do deletions at 2, 3, 4, 5, 6, 7, 8 (7 runs) and that will be more than enough to cover fri+sat
going with option 2 will be the most straightforward to implement. We'll just put a cap on the number processed. 
We could get fancy and literally count the bluesky points - add 1 per processed and 2 per delete.
but it's a little weird. regardless of the delete count, 345 or 359 we always process 3004 users from the list before we start timing out.

So I was hoping that at least the stats would work correctly, but that didn't work quite right either, it didn't seem to pull any of the stats from the dynamodb. So I'm gonna have to spend some time manually testing and getting this program right before redeploying. Which I should have done in the first place, just got too excited I guess.
The plus side is that now that I see it in action on the fri+sat day I have the great idea of testing the weekdays in super small batches (like 100) to make sure that the logging is working correctly. Idk why I was so resigned to just wait to test on fri+sat.
Another plus side is that 1. our error handling let the function calls survive, and 2. our S3 storage strategy and list handling and error handling all worked seemingly perfectly. So that's awesome.

Also looks like I only need 1gb of memory for the deletion, it's kind of time gated by the bluesky api calls more than anything anyway. 

So I'm gonna take the delete lambda offline for now, but luckily it wasn't going to be running anymore today and tomorrow anyway so I have plenty of time to get it back online. Well, not back online we'll be going to manual testing sunday.

And honestly I'm not particularly happy with how I'm logging the deletions anyway, now that I see it in action I think we can refine that a bit as well.

2025-02-28 1719
I was on my daily walk and reflecting on everything, still bemoaning how if I had a way to create kind of a function parameter I wouldn't have to do a full update to fine tune the performance of my microservices.
And then it hit me - The AWS SecretsManager secret! I can store some important parameters in there that the add and delete functions run on - the feed urls, how many follows to add (sat+week), how many posts to check (sat+week), how many posts to process before cutting off, how many deletes to process before cutting off, or a total "bluesky_api_points" to cut off at if we refactor that way.
I can keep those values in secrets manager, and if they ever need to be tuned they'll be updated in the secret instead of in the actual program so we don't need to do a full docker update!
That's gonna be sweet.
Especially in the far future if something weird happens like a feed I'm using gets deleted and needs to be replaced like that one time. It'll be so cool to just plug in a new feed anytime I want. Gotta review and see if there are any other parameters that I should use for all of the microservices.

Another thing I'm currently thinking about is reply-engagement. My like count is very high (like over 500) but replies are only in the single or double digits. Is the overlap between people who reply on posts and people who like posts a complete overlap? or are we leaving some replyers on the table? I'll need to test to investigate.

Finally, I'm considering a fifth microservice that would go through my own posts once a day and follow all the people who like my posts. I feel like that would be a high followback rate activity.

Maybe I'm not as done with this project as I thought I was lol. Definitely feels like the hard part is behind me though, I'm excited at still having a little room to optimize.

One last thing to mention is that we're in a bit of an awkward growth phase right now. With our follows/day finally maxxed out by automation, it feels weird that we're about to be at 15k followers but 45k following, brushing up on that 33% value I had originally come up with. But I'm confident that will even out over the next two weeks because our number of follows that aren't following back can only go down from here. And who knows how good the conversion rate will be now that we're automating.

reflecting on rate limit points again
5000 points per hour,
create is 3 points, update is 2 points, delete is 1 point. 
Again, reads do not take any points. But it makes sense that you can only do 2500 deletes. I don't know why 3500 were working before, maybe they were just taking a lot longer? So we just need to change the cap to 2500, but use the secretsmanager to bring it down to 2400 if necessary.
I guess it gives you a little leeway, which is why we were making it to 2650 deletes.
We'll go down to 2500 but store that value in the secret. Should be able to handle everything we're doing on 2400 even.

2025-03-02 0828
Refactoring the logging and exporting that value and fixing the bug that was preventing the stats from being logged looks like it was a success. It turns out I put the code to update stats in an else block that wouldn't ever be triggered unless the stats block already existed in the first place. Reuploaded to ECR and should be able to see it in action tomorrow.
A little worried about the status function performing correctly. 
But I kind of need an excuse to upload new versions of the add and status lambdas so I can change the logging filepath and implement the new secretsmanager to the adds. 
If it ain't broke don't fix it?

2025-03-04 1824
Things are going great with the adds and deletes. It was finally time for the weekly status run, but I encountered an error that I need to work through where I didn't realize that I had an uncommented reference to the huggingtoken value for no reason in that code. So gotta do a quick reupload which will let us change the logger filename.
On my walk today I was thinking about a problem I may need to address though - usually the required deletions from the status updates shouldn't be that large. Maybe like 20 people who stopped following. But if, somehow, I managed to follow 3000 people by hand within a week that didn't follow me back, that could rate limit me. 
Now that I write it out it sounds like I don't need to worry much about that happening. It crossed my mind because I was following people who liked my pinned post while I was donating blood yesterday.
But if you're repeating this project and you're not me, just fyi - don't do that lol.
But maybe I should be handling this so that we don't get timeouts. If I at least set a deletion cap, the remaining users would stick around until next week. We'll see how this run goes first though.

But one thing I wanted to start thinking about is the definition of "success" of this bot. There's this site:
https://vqv.app/
it shows the top 500 bluesky accounts by follower count.
To break into the top 500 you need 118,000 followers.
If you get to 150,000 followers you're in the top 350. Not sure if that will still be the case when we get to that amount, I assume that each of those will have gained followers.
But we've been pretty consistently getting 50 followers every two hours. It's crazy. 
THEORETICALLY to get the 100,000 followers to make it into the top 500, it will be 100,000 / 50 * 2 hours / 24 = 166.66 days. So that's the dream is to make it into the top 500 users.

2025-3-04 2021
ok good news and bad news
good news is the status update fix worked, and our run successfully completed without errors.
bad news is that the results are totally insane. it says we have 15170 users in our follows from over a week ago that still do not follow us and need to be pruned. It also says it was able to follow 5000 of them, failing to remove 10,170. Could that really be true? I did have 45,000 follows. Could it be deleting people that I follow at random??
Here's another thought - am I just refollowing people that I delete? Like a lot? I did get one message about it and blocked them. but what if it's happening by like 10s of thousands of users? That can't be, can it?
I'm kind of worried I need to take this service offline so that I can watch it run manually like the deletions. 

I think I have to think of a way to do this status update more frequently so that we can flush out all these old users in a way that's not going to contaminate our current stats. 

Our aggregation occurs at 4:20am, and deletions for the day happen shortly after, with no new adds.
So at 7am when our first run post deletions occur, then there should be Nobody followed that is not accounted for in an s3 bucket.
So if we pull all of the existing s3 objects for every day that is not today, then we have all of the follows that are currently 'percolating' for the week.
Which means we can create a hashset from THOSE users as the only users that we should not delete if they are not following us back. 
That way we're not running off this s3 bucket of just "whoever existed last week" and we can stay more up to date with status updates without interfering with the current runs.
also our time is at 8 mins already and we only had 45000 followers. it almost seems like if we double our followers count our lambda won't be able to run fast enough.
So do we move this process to manual runs permanently? No way.

I don't want to overreact here though. Theoretically, come next tuesday, if things are working as intended then we should still only have 10,170 left to prune, and we'll get rid of another 5000. 
then two weeks from now it will have balanced out. 

2025-03-05 2302 
ah dangit I saw my follow count drop 5000 and I realized I left the test trigger on the status function so now it's deleted 5000 at random from who knows. 
it did go from 10000 out of date to 33000 so it's detecting changes well? 
My stats will be a little messed up now but it's ok.
whoops. fixed now.

2025-03-06 0141
Received another complaint about multiple unfollowing and readdings.
I can't publicly apologize that I'm running code that's harassing these people, I've got to fix that.
So I'm going to have to keep a running hashset of all the people I've followed and load it in and out of S3 every time. 
I think I always wanted to do that but it was so hard to load in the whole database before.
How much data does 30million user ids take?
But THIS was the reason I was waiting for to update the add follows list. we'll get that fixed up and start a new logging chapter. 
Maybe instead of keeping a giant hash set of ids to screen out we should instead just block users so that we can't accidentally read them.
https://docs.bsky.app/blog/block-implementation
according to the documentation on block implementation, blocks do NOT remove follows. So to prune the follows list you would have to still remove the follow And block them, which I assume would cost twice as many api actions on the bluesky total, making it more difficult to get through our deletions.
But on the plus side it would handle everything on bluesky's end, not require us to load in and save a GIANT hashset of follows from all time, and it actually will technically speed up our process of adding follows because blocked users won't appear in our feeds when we generate them (supposedly).
So the question becomes - is it worth it for user experience to not add/readd people to never give someone the chance to follow again after just one week?
I'll have to think about this.
So it appears that there's a far less aggressive and more private option called "muting" 
"Muting a user hides their posts from your feeds. Mutes are private, and stored on your PDS. Muting a user is as easy as liking a post or following a user.
> client.mute(did)"
So we'll see if it's as expensive but I think this is a far better option than blocking because it will prevent the users from showing up in our feeds, which is the important part of not readding them with the add follows algorithm.

2025-03-06 2053
I created the necessary fixes for the status and delete microservices to mute every user that is deleted. Hopefully that fixes everything. Also managed to figure out how to get the number of muted accounts on my user so we can log that. We'll see at the next run at 2am if it works or not. Fingers crossed.
Would have liked to test a little more, but the command was a VERY basic addition, and I want to get it into production asap so I can stop harassing users. 
Fingers crossed! 
What's very awkward is that it's difficult to test if this works like it says it should work.
The bluesky documentation SAYS that muted users will not appear in my feeds. But if the muting still allows them into the api-generated feeds we're going to need to fix that. 
I guess the only way is to mute someone on a feed and see if they show up.
I did just do some testing manually with an existing feed and muting the top user. So it does look like it works that the posts won't show up at all, and so our add follows routine won't ever see them again.
Hopefully it will all go smoothly.

2025-03-07 0213
first round of deletions with muting was successful. Still deleted 2500 without timing out. Not sure why but only 2192 were muted. 
The only thing that went wrong was the logging stats - turns out that client.app.bsky.graph.get_mutes().mutes is the same as get_follows or get_feed, it gets pages of 50 at a time so the len was just 50. I'll have to fix that somehow. 
Still not sure about the 318 discrepancy. It can't be because it errored out.
The function took 9.5 mins, bumping the time up to 15 to max it out.
what's interesting is we hit 3 errors on mutes for invalid requests. But those accounts are successfully muted when I check them. 
Also i've introduced a slight bug where the mutes can cause the error count to go up above the acceptable amount, but it will never break out of the loop until an error is observed from the deletion call.
OH DEAR GOD I just noticed that we only had 4400 in the bucket to delete for last fri+sat and so none of our aggregates were working, I introduced that huggingface token bug 
i think I can fix this if I just run every day for the past week manually.
Ok crisis averted there.
But what in the world is going on with these mutes? I guess getting 90% of them muted is better than none.
The other reasons - ok so we mark them as no followback before 
* checking that there is no follow_uri, maybe I unfollowed a lot of them or there's some reason they don't make it into muting.
so I should move that try catch block out? no we still only want to mute if we're actually deleting the follow. So we'll leave it where it is. I think everything's ok.
The bad news that I've discovered is that there's no quickfunction to get the mutes count, you'd have to count them by page. Which is a little silly do I want to waste the time doing that? Probably not.

2025-03-07 1611
I realized I have made a terrible oversight. As I looked through my mentions and saw someone telling me to ~'fuck off and die' for following and unfollowing them, I looked at their profile and saw they weren't a cat poster at all. 
It made me realize that I needed to make sure that even though I've now made sure that I shouldn't be following and unfollowing Posters, the majority of my new follows are Likers, and those still show up in the list even if the account is muted.
So I need to make a tweak to make sure I'm not following anyone that I've muted.
luckily I've already figured that out, it's a pretty easy check since I'm iterating through the likes anyway.
print(len(likes))
for like in likes:
  print(like)
  print(like.actor.viewer.muted)  # this is the check we need to do to fix.
Did a quick test with a manual run of the microservice and it works as intended. Hopefully the reupload of the full docker image smoothly.


2025-03-10 0600
need to expand more on these:
1. getting stats, creating display of data over time
2. reflect on how the muting change actually improved the quality of our follows instead of spamming people who already did not want to follow. I've observed fewer follows per run, but nearly 200% followbacks. Hopefully the stats next week support this personal observation.
3. need to update the status-update function before it runs to gather how many users are currently muted.
4. To allow for better scaling - may need to break the status update 'monolithic microservice' into 4 chained lambdas. One lambda finishes, passes info to the next, and allows for 1 hour to process followers, follows, mutes, some other fourth thing, instead of trying to fit all of them into 15 minutes. Not an ideal thing to do, letting the runtime just triple, but it may be a necessary evil to allow for scale.

2025-03-11 0637
Something is seriously wrong with the delete code. I think that's why the status update is out of control with like 15000 people to delete, because the deletion runs aren't accurately doing what they're saying.
"2025-03-11 06:00:18.474992-04:00: Processed 1597 users from the list of 1598. 1 failures were encountered and need to be retried. From this batch of deletions 163 users followed back, 1434 did not follow back and were deleted, and 7 accounts no longer exist. 453 of the unfollowed users were successfully muted, with 0 errors.
  Follows count - now: 41438 | prev: 41890| mutes: 50. Successfully reuploaded remaining 1 users to s3."
I can confirm this number of 41.4k is accurate. Which means only 450 users were deleted. So good news is that the muted users (which have been far fewer than the number of deletions in almost every case) may be actually accurate to what we're doing.
But what's no good is taht the number of follows we're unfollowing is much lower. Like over a thousand lower now?

2025-03-11 1802
The status update needs to come fully offline while it gets reworked.
We had a full timeout where the function went to 15 minutes, didn't complete, retried because it errored out, and produced some weird behavior. 
Also I really don't like how I have the current logging output set up. 
We need to fully break this down more.
Also I would like to time it so the function is not occurring during prime time now it's happening at 6pm and that's a terrible time to do it.
Maybe the status update just needs to funnel the deletions into the deletions for tuesday or thursday

I also feel like I could optimize the runs during the weekdays - taking away a few runs in the early hours of the morning and adding them to the afternoon.

So how do we refactor?
break into three phases:
step 1. who follows us check - need to get the current list, compare to last list, see if any are missing so that we can delete them.
	this output should save as a map to s3 so that we can use it for phase 2.
step 2. who we follow, excluding who follows us - pull the set of who follows us, go through the users that we follow. If they're not following us we track them after a week and add them to the delete queue.
	We can add the users to the s3 bucket for the day that should already be deleted, then run subsequent operations. 
step 4. perform delete operations on all of the users that appear in the queue, adding them to the mute list. 
step 5. mute count - simply iterate through the full list of mutes

But the question is this - do we keep the once-a-week policy, or try to set it up to run more frequently?

2025-03-12 1327
I got rate limited on the delete function today. It makes it hard to see what's actually going on, but again it looks like even though it's saying it's processing 3800 and deleting 3400 only the 400 that are getting muted are being deleted.
For now I want to keep observing and just lower the cap for deletions to 2250 since it's rate limiting at 2300
What's very strange is that I could be getting rate limited on users that have already been deleted? otherwise how is the follow count not going down when the rate limit is still activating. 
I have a suspicion that this has to do with the follow-unfollow issue that we've repaired with the mutes. Maybe these 2000 users that are "deleted" are ones that were already deleted like yesterday or the day before. 
I need some kind of output to see what's going on in more detail.

2025-03-13 0645
Had one successful run at 2250, this time we had 2250 deletions and 1900 mutes. But the second deletion run an hour later got rate limited. 
So it looks like we need to scale back a bit more. if we have 5000 points per hour, 2 points for a del and 3 for a create we have 1666 creates and 2500 deletes per hour. Theoetically if we just divide by 0 we get 833 mutes and 1250 deletes
we had 10500 last week and that would be covered in 7 runs, and we have 8 on fridays. we'll reduce to 1750 just in case we have like 14000, but there's no way we had that many. 
I'm worried a bit about hitting the full daily rate limit now, but we'll see how it goes.

2025-03-14 0914
lowering to 1750 managed to get through all of the runs for fri+sat without erroring out. Consistently got about 1600 mutes per run, and that was also the number of follows that were actually deleted even though 1750 were claimed. Still need to do some debugging and better logging to represent this, but at least it seems like it's under control now.

2025-03-16 1454
With mutes the amount of follows I'm generating through runs has decreased dramatically, Usually about half target. Even on caturday. It looks like we may have peaked and we're on a downswing, unfortunately.
The pluses of this though, are that once we can safely lower the deletions to 1550 to try to get it in line with the mute restriction and fully align those. 
Additionally it's becoming more and more feasible to just write a 'purging' code. No more checking followers to see who stopped following, just pull all the people we recently followed from today and this past week, see who isn't following, and unfollow them. 
We're still at a decent follower-follow ratio right now (22.5k : 37.7k) but now that we're looking like we're not going to reach the dream of 50k/100k where they'll even out, I think I want to even up the numbers as much as possible.
This should probably be a manual run rather than a lambdafied run, gotta put it into a delete queue that we'd run manually until it's finished.
I'm also tossing around the idea of following dog-posters, or animal posters in general. maybe it's time to investigate some more popular feeds?
I don't know. The temptation to fight to keep number going up is kind of what's wrong with all of tech. Sometimes exponential growth just isn't sustainable. As awesome as it would be for my resume to make it into the top 1000 users on bluesky it doesn't seem to be in the cards.

2025-03-18 1028
Today's big deletion run had 1673/1750 successful mutes out of the deletions, so I think things are working as intended and we're not hitting the cap. We're just getting fewer and fewer mute duplicates now that we've been running the mute for nearly a week.
I'm curious to see the actual stats of our follower rate gain. With so many fewer follows it FEELS like we're not growing nearly as rapidly, but when I look at the follower counts it seems we're still getting 20-40 between runs, so it can't be nearly as bad as I expected. However the follows during the slow hours have crawled down to like 10 every 2 hours so that's something. 
I still haven't decided what to do about the status update purges. I'm tempted just to just use the database version of the code for old time's sake.

2025-03-19 1639
Not going to bother doing something fancier. Returned to the database and input all of our current follows. In exactly one week I'll be able to get a new status update that covers everything.
Here's something really fascinating though - when I got the status update it grabbed about 32.5k users to put into the database. But my current follow count is 37.5k. I'm missing about 5000.
So confused, I queried the database, got the users, and tried to see if the status update ran incorrectly. It didn't get any new users, it said previous follows were 37.5k but no new users to add.
Why? I suspect that these users are the ones that have blocked us over the past few months. Which would be almost impressive lol getting blocked by over 5000 people. But since the status update can't find them, we need another way to get rid of them. 
So it turns out you can't even see these follows when you iterate completely through the follows list. I tried to iterate through all 37500 and add everything to a list, but the list came out to a size of 32500. 
I've been testing with a bluesky handle of a user that accused me of content thieving and blocked me when I denied it.
I've found a third party tool that requires you plug in your username and password, but i'd prefer not to do that. 
But it would be extremely beneficial to get these users out of my follows list, they account for 1/3 of the surplus (follows more than following) right now.
Not sure how to address with the current apis, so I've put in a support ticket.

Also as a side note, today's deletions were even more successful - out of the 1750 deletions in the first run, 1712 were successfully muted. So it looks like things are going good on that side of things. I just have to find a way to purge these blockers and purge the ones that have slipped through the cracks and things should be looking much prettier account-numbers wise.

2025-03-25 1533 
interesting website someone showed me to see who's blocking you - https://clearsky.app/tyrowo.com/labeled
unfortunately though, it seems that bluesky is not going to respond to my issue from last week about the API to remove blocked users from our followed list.
Luckily we don't have to worry about re-following them because they're blocked or suspended or whatever, but someone created a Typescript tool here:
https://cleanfollow-bsky.pages.dev/
It seems that it manually checks the pages as your logged in user to see the response status of the account, flags any ones that aren't normally accessible, and I couldn't figure out how it was able to unfollow them. I guess just once you see the status of the account you already have the did so you can just unfollow the user maybe? Might need to look through their github again.
I wanted to write some of that code myself but instead I've decided to be lazy, change my password to a temporary, log in to this random person's tool, and have them do it for me. And it worked great.
It found and deleted ~ 5122. The runtime to check all the follows was a long time, about two hours for 35000 users. So I missed a run, and I think I'm going to have to miss this 4pm from timeouts, so we've lost two prime time runs, but that's ok. #worth
I think we only deleted 5000 users like old times - I'm definitely fully rate limited right now, and from my follower math it looks like it didn't delete the last 
So maybe I'll go back and do another cleanup in a week or two, when I'm up in the middle of the night and we can skip a 2am run or something. Or even better, when we're done with the deletions for the day and can miss an 8am run.

Anyway. We hit an incredibly huge milestone this morning imo. 25000 followers. So with the cleanup bringing us down from 36k to 31k we're closer than ever to evening out.
Excited to see how the status-update goes tomorrow. I think surely there are another couple thousand that have just fallen through the cracks of our delete code when I've had mistakes going. So we might be all evened up tomorrow!

2025-03-26 11:11
well good news and bad, I suppose. The good news is that there were only 206 follows that were 'unaccounted' for by previous runs. Which means that we have more good news that we've done an exceptional job of keeping track of our follows throughout both processes.
Bad news is not that bad - with only 206 deletions, we're not going to be evening out our follower-follow ratio like I'd hoped. there are another ~180 we can get rid of by running the blocked-user-removal tool again, though.
What that means is that we're at  25237 followers and following 29784 and that ~4500 gap is just going to be the flux of whoever we've added for the week. 
I'm just really impressed that everyone else is accounted for, that's pretty wild.
